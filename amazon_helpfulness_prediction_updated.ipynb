{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import gzip\n",
    "import math\n",
    "import nltk\n",
    "import string \n",
    "import scipy\n",
    "import ast\n",
    "from nltk.corpus import cmudict\n",
    "from nltk.corpus import stopwords\n",
    "import sklearn.metrics as skmetrics\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, linear_model\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import ensemble\n",
    "from nltk.corpus import cmudict \n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from gzip file\n",
    "def read_gzip(filename):\n",
    "    for line in gzip.open(filename):\n",
    "        yield eval(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get (positive/negative) opinion words from corpus\n",
    "def get_opinion_words(filename):\n",
    "    with open(filename,'r',encoding = \"ISO-8859-1\") as f:\n",
    "        for line in f:\n",
    "            yield line\n",
    "            \n",
    "positive_words = set()\n",
    "negative_words = set()\n",
    "\n",
    "for pword in get_opinion_words('positive-words.txt'):\n",
    "    positive_words.add(pword[:-2])\n",
    "    \n",
    "for nword in get_opinion_words('negative-words.txt'):\n",
    "    negative_words.add(nword[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute number of syllables for a given word\n",
    "d = cmudict.dict() \n",
    "def nsyl(word):\n",
    "    max_syl = 0\n",
    "    if word.lower() in d:\n",
    "        for syl_group in d[word.lower()]:\n",
    "            tot_syl = 0\n",
    "            for syl in syl_group:\n",
    "                if str(syl[-1]).isdigit():\n",
    "                    tot_syl += 1\n",
    "            max_syl = max(max_syl,tot_syl)\n",
    "    return max_syl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pruning\n",
    "\n",
    "- Removed data points with greater than 150 votes as the test data has very few data points in that range. \n",
    "- Prepared two separate datasete one for highly votes reviews (>10) and one for reviews that recieved low votes (<10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "data = pd.read_csv('Automotive.csv')\n",
    "data.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = {'reviewerID':0, 'itemID':1, 'reviewerName':2, 'helpful':3, 'reviewText':4, 'rating':5, 'summary':6, 'unixReviewTime':7, 'reviewTime':8}\n",
    "low_dataset = []\n",
    "high_dataset = []\n",
    "\n",
    "for line in data.values:\n",
    "    line[index['helpful']] = ast.literal_eval(line[index['helpful']])\n",
    "    if int(line[index['helpful']][1]) <= 150 and int(line[index['helpful']][1]) > 10 :\n",
    "        high_dataset.append(line)\n",
    "    elif int(line[index['helpful']][1]) <= 10 and int(line[index['helpful']][1]) >= 1 :\n",
    "        low_dataset.append(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute USER specific data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user_ratings_dict = defaultdict(list)\n",
    "train_all_helpful_review = []\n",
    "train_user_helpful_review_dict = defaultdict(list)\n",
    "train_user_review_content_dict = defaultdict(list)\n",
    "\n",
    "for data_point in data.values:   \n",
    "    if (data_point[index['helpful']])[1] == 0:\n",
    "        continue\n",
    "      \n",
    "    data_point[index['helpful']] = ast.literal_eval(data_point[index['helpful']])\n",
    "    train_user_ratings_dict[data_point[index['reviewerID']]].append(int(data_point[index['rating']]))\n",
    "    train_all_helpful_review.append(data_point[index['helpful']])\n",
    "    train_user_helpful_review_dict[data_point[index['reviewerID']]].append(data_point[3])    \n",
    "    train_user_review_content_dict[data_point[index['reviewerID']]].append(data_point[index['reviewText']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute ITEM specific data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Initialize ITEM specific data structures\n",
    "train_items_ratings_dict = defaultdict(list)\n",
    "train_user_purchased_items_dict = defaultdict(list)\n",
    "for data_point in data.values:\n",
    "    user = data_point[index['reviewerID']]\n",
    "    item = data_point[index['itemID']]\n",
    "    train_user_purchased_items_dict[user].append(item)\n",
    "    train_items_ratings_dict[item].append(int(data_point[index['rating']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_average_items_ratings_dict = {}\n",
    "train_item_review_count = {}\n",
    "for item in train_items_ratings_dict:\n",
    "    train_average_items_ratings_dict[item] = np.mean(train_items_ratings_dict[item])\n",
    "    train_item_review_count[item] = len(train_items_ratings_dict[item])\n",
    "print (\"Average ratings computed for \" + str(len(train_average_items_ratings_dict.values())) + \" items\")\n",
    "print (\"Number of reviews computed for \" + str(len(train_item_review_count.values())) + \" items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute OVERALL average helpfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_average_helpfulness = sum([x[0] for x in train_all_helpful_review]) * 1.0 / sum([\n",
    "        x[1] for x in train_all_helpful_review])\n",
    "print (\"Average Helpfulness : %s\" % global_average_helpfulness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USER SPECIFIC FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_review_experience_count_feature(user_review_text_dict):\n",
    "    user_review_experience = {}\n",
    "    # Compute number of reviews given by a user\n",
    "    for user in user_review_text_dict:\n",
    "        user_review_experience[user] = len(user_review_text_dict[user])\n",
    "    return user_review_experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_average_ratings_feature(user_ratings_dict, train_global_average_ratings):\n",
    "    user_average_ratings = {}\n",
    "    # Compute average ratings given by user or fill with global average ratings\n",
    "    for user in user_ratings_dict:\n",
    "        total_user_ratings = len(train_user_ratings_dict[user])\n",
    "        if total_user_ratings > 0:\n",
    "            user_average_ratings[user] = sum(train_user_ratings_dict[user]) * 1.0/total_user_ratings\n",
    "        else:\n",
    "            user_average_ratings[user] = train_global_average_ratings\n",
    "    return user_average_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_average_helpfulness_feature(train_user_helpful_review_dict, train_global_average_helpfulness):\n",
    "    user_average_helpfulness = {}\n",
    "    # Compute average helpfulness of users or fill with global average helpfulness values\n",
    "    for user in train_user_helpful_review_dict:\n",
    "        total_user_helpful_review = sum([x['outOf'] for x in train_user_helpful_review_dict[user]])\n",
    "        if total_user_helpful_review > 0:\n",
    "            user_average_helpfulness[user] = sum(\n",
    "                [x['nHelpful'] for x in train_user_helpful_review_dict[user]]) * 1.0 / total_user_helpful_review\n",
    "        else:\n",
    "            user_average_helpfulness[user] = train_global_average_helpfulness\n",
    "\n",
    "    return user_average_helpfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_rating_deviation_feature(user_ratings_dict, average_items_ratings_dict, user_purchased_items_dict):\n",
    "    user_rating_deviation = {}\n",
    "    for user in user_ratings_dict:\n",
    "        user_rating_deviation[user] = np.mean([(user_ratings_dict[user] - average_items_ratings_dict[item])**2 \n",
    "         for item in user_purchased_items_dict[user] if item in average_items_ratings_dict])\n",
    "    return user_rating_deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# FEATURE : USER REVIEW EXPERIENCE \n",
    "train_user_review_experience = get_user_review_experience_count_feature(train_user_review_content_dict)\n",
    "print (\"Extracted user review experience for \" + str(len(train_user_review_experience.values())) + \" users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATASET DEPENDENT FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_helpfulness(dataset):\n",
    "    data_average_helpfulness = []\n",
    "    for data_point in dataset:\n",
    "        data_average_helpfulness.append(global_average_helpfulness)\n",
    "    return data_average_helpfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rating(dataset):\n",
    "    data_ratings = []\n",
    "    for data_point in dataset:\n",
    "        data_ratings.append(int(data_point[index['rating']]))\n",
    "    return data_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_square_rating(dataset):\n",
    "    data_ratings = []\n",
    "    for data_point in dataset:\n",
    "        data_ratings.append(int(data_point[index['rating']])**2)\n",
    "    return data_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_ratings(dataset):\n",
    "    data_ratings = []\n",
    "    for data_point in dataset:\n",
    "        data_ratings.append(int(data_point[index['rating']]))\n",
    "    data_ratings = np.array(data_ratings)\n",
    "    return np.log(data_ratings.max() + 1 - data_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_helpfulness_votes(dataset):\n",
    "    data_helpfulness_votes = []\n",
    "    for data_point in dataset:\n",
    "        #line[3] = ast.literal_eval(line[3])\n",
    "        if data_point[index['helpful']] is not list:\n",
    "            votes_json = json.loads(str(data_point[index['helpful']]))\n",
    "        else:\n",
    "            votes_json = data_point[index['helpful']]\n",
    "        data_helpfulness_votes.append(np.log(votes_json[1] + 1))\n",
    "    return data_helpfulness_votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review_word_count(dataset):\n",
    "    data_review_word_count = []\n",
    "    for data_point in dataset:\n",
    "        data_review_word_count.append(np.log(len(data_point[index['reviewText']].lower().split())+1))\n",
    "    return data_review_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_count(dataset):\n",
    "    data_review_sentence_count = []\n",
    "    for data_point in dataset:\n",
    "        data_review_sentence_count.append(np.log(len(data_point[index['reviewText']].lower().split('.'))+1))\n",
    "    return data_review_sentence_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review_allcaps_count(dataset):\n",
    "    data_review_word_allcaps_count = []\n",
    "    for data_point in dataset:\n",
    "        data_review_word_allcaps_count.append(np.log(len([\n",
    "                        word for word in data_point[index['reviewText']].split() if word.isupper()])+1))\n",
    "    return data_review_word_allcaps_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review_char_count(dataset):\n",
    "    data_review_char_count = []\n",
    "    for data_point in dataset:\n",
    "        data_review_char_count.append(np.log(sum([len(word) for word in data_point[index['reviewText']].lower().split()])+1))\n",
    "    return data_review_char_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review_specialchar_count(dataset):\n",
    "    data_review_specialchar_count = []\n",
    "    for data_point in dataset:\n",
    "        data_review_specialchar_count.append(len([word for word in data_point[index['reviewText']].lower().split() \n",
    "                                           if \"!\" in word or \"?\" in word ]))\n",
    "    return data_review_specialchar_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_rating_deviation(dataset):\n",
    "    data_item_rating_deviation = []\n",
    "    for data_point in dataset:\n",
    "        rating = int(data_point[index['rating']])\n",
    "        score = 0\n",
    "        if rating==1 or rating==5:\n",
    "            rating = 10\n",
    "        elif rating==2 or rating==4:\n",
    "            rating = 8\n",
    "        elif rating==3:\n",
    "            rating = 5\n",
    "        data_item_rating_deviation.append(rating)\n",
    "        #data_item_rating_deviation.append(np.abs(int(data_point[index['rating']]) - train_average_items_ratings_dict[data_point[index['itemID']]]))\n",
    "    return data_item_rating_deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flesch_reading_ease_score(dataset):\n",
    "    data_review_flesch_reading_score = []\n",
    "    for data_point in dataset:\n",
    "        total_words = len(data_point[index['reviewText']].lower().split())\n",
    "        total_sent = len(data_point[index['reviewText']].lower().split('.'))\n",
    "        total_syllable = sum([nsyl(word) for word in data_point[index['reviewText']].lower().split()])\n",
    "        data_review_flesch_reading_score.append(206.835 - (1.015*(total_words* 1.0/(1+total_sent))) - \\\n",
    "                                                (84.6*(total_syllable * 1.0/(1+total_words))))        \n",
    "    return data_review_flesch_reading_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_readability_index(dataset):\n",
    "    data_review_readability_index = []\n",
    "    for data_point in dataset:\n",
    "        total_char = sum([len(word) for word in data_point[index['reviewText']].lower().split()])\n",
    "        total_words = len(data_point[index['reviewText']].lower().split())\n",
    "        total_sent = len(data_point[index['reviewText']].lower().split('.'))\n",
    "        data_review_readability_index.append((4.71*(total_char*1.0/(1+total_words))) + \n",
    "                                            (0.5*(total_words*1.0/(1+total_sent))) - 21.43)\n",
    "    return data_review_readability_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_word_count(dataset):\n",
    "    data_summary_word_count = []\n",
    "    for data_point in dataset:\n",
    "        # Sudi suggestion - normalize by review size\n",
    "        data_summary_word_count.append(np.log(len([word for word in data_point[index['summary']].lower().split()])+1))\n",
    "    return data_summary_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_char_count(dataset):\n",
    "    data_summary_char_count = []\n",
    "    for data_point in dataset:\n",
    "        data_summary_char_count.append(np.log(sum([len(word) for word in data_point[index['summary']].lower().split()])+1))\n",
    "    return data_summary_char_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_allcaps_count(dataset):\n",
    "    data_summary_word_allcaps_count = []\n",
    "    for data_point in dataset:\n",
    "        data_summary_word_allcaps_count.append(sum([1 for word in data_point[index['summary']].split() if word.isupper()]))\n",
    "    return data_summary_word_allcaps_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_specialchar_count(dataset):\n",
    "    data_summary_specialchar_count = []\n",
    "    for data_point in dataset:\n",
    "        data_summary_specialchar_count.append(len([word for word in data_point[index['summary']].lower().split() \n",
    "                                           if \"!\" in word or \"?\" in word ]))\n",
    "    return data_summary_specialchar_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review_sentiment_score(dataset):\n",
    "    data_review_pos_sentiment = []\n",
    "    data_review_neg_sentiment = []\n",
    "    data_review_obj_sentiment = []\n",
    "    \n",
    "    for data_point in dataset:\n",
    "        review_text = data_point[index['reviewText']].lower().split()\n",
    "        data_review_pos_sentiment.append(sum([\n",
    "                    sum([x.pos_score() for x in swn.senti_synsets(word.lower())]) for word in review_text]))\n",
    "        data_review_neg_sentiment.append(sum([\n",
    "                    sum([x.neg_score() for x in swn.senti_synsets(word.lower())]) for word in review_text]))\n",
    "        data_review_obj_sentiment.append(sum([\n",
    "                    sum([x.obj_score() for x in swn.senti_synsets(word.lower())]) for word in review_text]))\n",
    "    return data_review_pos_sentiment, data_review_neg_sentiment, data_review_obj_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_sentiment_score(dataset):\n",
    "    data_summary_pos_sentiment = []\n",
    "    data_summary_neg_sentiment = []\n",
    "    data_summary_obj_sentiment = []\n",
    "    for data_point in dataset:\n",
    "        summary_text = data_point[index['summary']].lower().split()\n",
    "        data_summary_pos_sentiment.append(sum([\n",
    "                    sum([x.pos_score() for x in swn.senti_synsets(word.lower())]) for word in summary_text]))\n",
    "        data_summary_neg_sentiment.append(sum([\n",
    "                    sum([x.neg_score() for x in swn.senti_synsets(word.lower())]) for word in summary_text]))\n",
    "        data_summary_obj_sentiment.append(sum([\n",
    "                    sum([x.obj_score() for x in swn.senti_synsets(word.lower())]) for word in summary_text]))        \n",
    "        \n",
    "    return data_summary_pos_sentiment, data_summary_neg_sentiment, data_summary_obj_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_review_experience(dataset):\n",
    "    data_user_review_experience = []\n",
    "    for data_point in dataset:\n",
    "        if data_point[index['reviewerID']] in train_user_review_experience:\n",
    "            if train_user_review_experience[data_point[index['reviewerID']]] >= 5:\n",
    "                data_user_review_experience.append(np.log(train_user_review_experience[data_point[index['reviewerID']]]+1))\n",
    "            else:\n",
    "                data_user_review_experience.append(0)\n",
    "        else:\n",
    "            data_user_review_experience.append(0)\n",
    "    return data_user_review_experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_id(dataset):\n",
    "    category_0 = []\n",
    "    category_1 = []\n",
    "    category_2 = []\n",
    "    category_3 = []\n",
    "    category_4 = []\n",
    "\n",
    "    for data_point in dataset:\n",
    "        cat_id = data_point['categoryID']\n",
    "        if cat_id == 0:\n",
    "            category_0.append(1)\n",
    "            category_1.append(0)\n",
    "            category_2.append(0)\n",
    "            category_3.append(0)\n",
    "            category_4.append(0)\n",
    "        if cat_id == 1:\n",
    "            category_0.append(0)\n",
    "            category_1.append(1)\n",
    "            category_2.append(0)\n",
    "            category_3.append(0)\n",
    "            category_4.append(0)\n",
    "        if cat_id == 2:\n",
    "            category_0.append(0)\n",
    "            category_1.append(0)\n",
    "            category_2.append(1)\n",
    "            category_3.append(0)\n",
    "            category_4.append(0)\n",
    "        if cat_id == 3:\n",
    "            category_0.append(0)\n",
    "            category_1.append(0)\n",
    "            category_2.append(0)\n",
    "            category_3.append(1)\n",
    "            category_4.append(0)\n",
    "        if cat_id == 4:\n",
    "            category_0.append(0)\n",
    "            category_1.append(0)\n",
    "            category_2.append(0)\n",
    "            category_3.append(0)\n",
    "            category_4.append(1)\n",
    "    return category_0, category_1, category_2, category_3, category_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review_year(dataset):\n",
    "    review_years = []\n",
    "    for data_point in dataset:\n",
    "        year = int(data_point[index['reviewTime']].split(',')[-1].strip())\n",
    "        review_years.append(year - 2005)\n",
    "    return review_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_average_ratings(dataset):\n",
    "    user_average_ratings = []\n",
    "    for data_point in dataset:\n",
    "        if data_point[index['reviewerID']] in train_user_ratings:\n",
    "            user_average_ratings.append(train_user_ratings[data_point[index['reviewerID']]])\n",
    "        else:\n",
    "            user_average_ratings.append(train_global_average_ratings)\n",
    "    return user_average_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review_stopwords(dataset):\n",
    "    review_stopwords = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for data_point in dataset:\n",
    "        review = data_point[index['reviewText']].lower().split()\n",
    "        review_stopwords.append(np.log(sum([1 if word in stop_words else 0 for word in review])+1))\n",
    "    return review_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review_non_stopwords(dataset):\n",
    "    review_nonstopwords = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for data_point in dataset:\n",
    "        review = data_point[index['reviewText']].lower().split()\n",
    "        review_nonstopwords.append(np.log(sum([1 if word not in stop_words else 0 for word in review])+1))\n",
    "    return review_nonstopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review_positive_words(dataset):\n",
    "    review_positive_words = []\n",
    "    for data_point in dataset:\n",
    "        review = data_point[index['reviewText']].lower().split()\n",
    "        review_positive_words.append(np.log(sum([1 if word in positive_words else 0 for word in review])+1))\n",
    "    return review_positive_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review_negative_words(dataset):\n",
    "    review_negative_words = []\n",
    "    for data_point in dataset:\n",
    "        review = data_point[index['reviewText']].lower().split()\n",
    "        review_negative_words.append(np.log(sum([1 if word in negative_words else 0 for word in review])+1))\n",
    "    return review_negative_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review_posneg_diff_words(dataset):\n",
    "    review_posneg_diff_words = []\n",
    "    for data_point in dataset:\n",
    "        review = data_point[index['reviewText']].lower().split()\n",
    "        neg = sum([1 if word in negative_words else 0 for word in review])\n",
    "        pos = sum([1 if word in positive_words else 0 for word in review])\n",
    "        review_posneg_diff_words.append(abs(neg-pos)+1)\n",
    "        #review_posneg_diff_words.append(np.log(abs(neg-pos)+1))\n",
    "    return review_posneg_diff_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_positive_words(dataset):\n",
    "    review_positive_words = []\n",
    "    for data_point in dataset:\n",
    "        review = data_point[index['summary']].lower().split()\n",
    "        review_positive_words.append(sum([1 if word in positive_words else 0 for word in review])+1)\n",
    "        #review_positive_words.append(np.log(sum([1 if word in positive_words else 0 for word in review])+1))\n",
    "    return review_positive_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_negative_words(dataset):\n",
    "    review_negative_words = []\n",
    "    for data_point in dataset:\n",
    "        review = data_point[index['summary']].lower().split()\n",
    "        review_negative_words.append(sum([1 if word in negative_words else 0 for word in review])+1)\n",
    "        #review_negative_words.append(np.log(sum([1 if word in negative_words else 0 for word in review])+1))\n",
    "    return review_negative_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_posneg_diff_words(dataset):\n",
    "    review_posneg_diff_words = []\n",
    "    for data_point in dataset:\n",
    "        review = data_point[index['summary']].lower().split()\n",
    "        neg = sum([1 if word in negative_words else 0 for word in review])\n",
    "        pos = sum([1 if word in positive_words else 0 for word in review])\n",
    "        review_posneg_diff_words.append(abs(neg-pos)+1)\n",
    "        #review_posneg_diff_words.append(np.log(abs(neg-pos)+1))\n",
    "    return review_posneg_diff_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_review_count(dataset):\n",
    "    item_review_count = []\n",
    "    for data_point in dataset:\n",
    "        if data_point[index['itemID']] in train_item_review_count:\n",
    "            item_review_count.append(train_item_review_count[data_point[index['itemID']]])\n",
    "        else:\n",
    "            item_review_count.append(0)\n",
    "    return item_review_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rating_category(dataset):\n",
    "    bad = []\n",
    "    ok = []\n",
    "    good = []\n",
    "    \n",
    "    for data_point in dataset:\n",
    "        r = data_point[index['rating']]\n",
    "        if r < 2.0:\n",
    "            bad.append(1)\n",
    "            ok.append(0)\n",
    "            good.append(0)\n",
    "        if r >= 2.0 and r< 4.0:\n",
    "            bad.append(0)\n",
    "            ok.append(1)\n",
    "            good.append(0)\n",
    "        if r >= 4.0:\n",
    "            bad.append(0)\n",
    "            ok.append(0)\n",
    "            good.append(1)\n",
    "    return bad, ok, good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outof_group(dataset):\n",
    "    outof_low = []\n",
    "    outof_mid1 = []\n",
    "    outof_mid2 = []\n",
    "    outof_high = []\n",
    "    for data_point in dataset:\n",
    "        out_of = int((data_point[index['helpful']])[1])\n",
    "        if out_of < 10:\n",
    "            outof_low.append(1)\n",
    "            outof_mid1.append(0)\n",
    "            outof_mid2.append(0)\n",
    "            outof_high.append(0)\n",
    "            \n",
    "        elif out_of < 40:\n",
    "            outof_low.append(0)\n",
    "            outof_mid1.append(2)\n",
    "            outof_mid2.append(0)\n",
    "            outof_high.append(0)\n",
    "        \n",
    "        elif out_of < 80:\n",
    "            outof_low.append(0)\n",
    "            outof_mid1.append(0)\n",
    "            outof_mid2.append(3)\n",
    "            outof_high.append(0)\n",
    "            \n",
    "        elif out_of >= 80:\n",
    "            outof_low.append(0)\n",
    "            outof_mid1.append(0)\n",
    "            outof_mid2.append(0)\n",
    "            outof_high.append(4)\n",
    "            \n",
    "    return outof_low, outof_mid1, outof_mid2, outof_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unixtime(dataset):\n",
    "    unixTime = []\n",
    "    user_unix_time = []\n",
    "    for data_point in dataset:\n",
    "        unixTime.append(data_point[index['unixReviewTime']])\n",
    "    \n",
    "    max_unix = max(unixTime)\n",
    "    min_unix = min(unixTime)\n",
    "    \n",
    "    for data_point in dataset:\n",
    "        user_unix_time.append(np.log(max_unix - data_point[index['unixReviewTime']] + 1))\n",
    "    return unixTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ideal_vector(high_dataset, low_dataset):\n",
    "    dataset = np.concatenate((high_dataset, low_dataset))\n",
    "    reviews = dataset[:,index['reviewText']]\n",
    "    vec = TfidfVectorizer(min_df=0.02, ngram_range=(1,3), analyzer='word', stop_words=stopwords.words('english'))\n",
    "    vec.fit(reviews)\n",
    "    reviews = vec.transform(reviews)\n",
    "    reviews = reviews.toarray()\n",
    "    ideal_vector = [0.]*reviews.shape[1]\n",
    "    for vector in reviews:\n",
    "        ideal_vector = np.add(ideal_vector, vector)\n",
    "\n",
    "    ideal_vector = np.divide(ideal_vector, reviews.shape[0])\n",
    "    return vec, ideal_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_similarity_review(dataset, tf_idf_vectoriser, ideal_vector):\n",
    "    cosine_similarity_score = []\n",
    "    for data_point in dataset:\n",
    "        review = data_point[index['reviewText']]\n",
    "        review = tf_idf_vectoriser.transform([review])\n",
    "        review = review.toarray()\n",
    "        similarity_score = float(np.linalg.norm(np.dot(ideal_vector, review[0]))/(np.linalg.norm(ideal_vector)*np.linalg.norm(review)))\n",
    "    \n",
    "        if np.isnan(similarity_score):\n",
    "            #print(data_point[inde])\n",
    "            similarity_score = 0.0\n",
    "        cosine_similarity_score.append(similarity_score)\n",
    "    print(cosine_similarity_score)\n",
    "    return cosine_similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Ideal Vector\n",
    "tf_idf_vectoriser, ideal_vector = get_ideal_vector(high_dataset, low_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpfulness Prediction Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Training Feature Set for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(dataset):\n",
    "\n",
    "    ratings = get_rating(dataset)\n",
    "    print (\"Ratings extracted..\" + str(np.matrix(ratings).shape))\n",
    "\n",
    "    square_ratings = get_square_rating(dataset)\n",
    "    print (\"Squared Ratings extracted.. \" + str(np.matrix(square_ratings).shape))\n",
    "\n",
    "    total_helpfulness_votes = get_helpfulness_votes(dataset)\n",
    "    print (\"Helpfulness extracted..\"+ str(np.matrix(total_helpfulness_votes).shape))\n",
    "\n",
    "    review_word_count = get_review_word_count(dataset)\n",
    "    print (\"Review word count extracted..\"+ str(np.matrix(review_word_count).shape))\n",
    "    \n",
    "    review_sentence_count = get_sentence_count(dataset)\n",
    "    print (\"Review Sentence count extracted..\"+ str(np.matrix(review_sentence_count).shape))\n",
    "    \n",
    "    review_word_allcaps_count = get_review_allcaps_count(dataset)\n",
    "    print (\"Review word all caps extracted..\"+ str(np.matrix(review_word_allcaps_count).shape))\n",
    "    \n",
    "    review_char_count = get_review_char_count(dataset)\n",
    "    print (\"Review character count extracted..\"+ str(np.matrix(review_char_count).shape))\n",
    "\n",
    "    item_rating_deviation = get_item_rating_deviation(dataset)\n",
    "    print (\"Item rating deviation extracted..\"+ str(np.matrix(item_rating_deviation).shape))\n",
    "    \n",
    "    summary_word_count = get_summary_word_count(dataset)\n",
    "    print (\"Summary word count extracted..\"+ str(np.matrix(summary_word_count).shape))\n",
    "\n",
    "    summary_word_allcaps_count = get_summary_allcaps_count(dataset)\n",
    "    print (\"Summary word all caps extracted..\"+ str(np.matrix(summary_word_allcaps_count).shape))\n",
    "\n",
    "    user_review_experience = get_user_review_experience(dataset)\n",
    "    print (\"User review experience extracted..\" + str(np.matrix(user_review_experience).shape))\n",
    "\n",
    "    review_readability_index = get_readability_index(dataset)\n",
    "    print (\"Review readability score extracted..\"+ str(np.matrix(review_readability_index).shape))\n",
    "    \n",
    "    review_posneg_diff = get_review_posneg_diff_words(dataset)\n",
    "    print (\"Review positive-negative difference extracted..\" + str(np.matrix(review_posneg_diff).shape))\n",
    "    \n",
    "    review_stopwords = get_review_stopwords(dataset)\n",
    "    print (\"Review stop words extracted.. \" + str(np.matrix(review_stopwords).shape))\n",
    "    \n",
    "    summary_neg_words = get_summary_negative_words(dataset)\n",
    "    print (\"Summary negative words extracted..\" + str(np.matrix(summary_neg_words).shape))\n",
    "\n",
    "    summary_specialchar_count = get_summary_specialchar_count(dataset)\n",
    "    print (\"Summary special character count extracted..\"+ str(np.matrix(summary_specialchar_count).shape))\n",
    "        \n",
    "    summary_pos_words = get_summary_positive_words(dataset)\n",
    "    print (\"Summary positive words extracted..\" + str(np.matrix(summary_pos_words).shape))\n",
    "      \n",
    "    summary_posneg_words = get_summary_posneg_diff_words(dataset)\n",
    "    print (\"Summary posneg difference extracted.. \" + str(np.matrix(summary_posneg_words).shape))\n",
    "\n",
    "    rating_bad, rating_ok, rating_good = get_rating_category(dataset)\n",
    "    print (\"Extracted rating category..\" + str(np.matrix(rating_bad).shape))\n",
    "\n",
    "    review_nonstopwords = get_review_non_stopwords(dataset)\n",
    "    print (\"Review non-stop words extracted..\" + str(np.matrix(review_nonstopwords).shape))\n",
    "    \n",
    "    review_year = get_review_year(dataset)\n",
    "    print (\"Review years extracted..\" + str(np.matrix(review_year).shape))\n",
    "    \n",
    "    outOf_low, outOf_mid1, outOf_mid2, outOf_high = get_outof_group(dataset)\n",
    "    print (\"Extracted one-hot encoded outOf categories..\" + str(np.matrix(outOf_low).shape))\n",
    "    \n",
    "    unixTime = get_unixtime(dataset)\n",
    "    print (\"Extracted unix time of review..\" + str(np.matrix(unixTime).shape))\n",
    "     \n",
    "    review_pos_words = get_review_positive_words(dataset)\n",
    "    print(\"Review positive words extracted..\" + str(np.matrix(review_pos_words).shape))\n",
    "    \n",
    "    review_neg_words = get_review_negative_words(dataset)\n",
    "    print(\"Review negative words extracted..\" + str(np.matrix(review_neg_words).shape))\n",
    "    \n",
    "    data_review_flesch_reading_score = get_flesch_reading_ease_score(dataset)\n",
    "    print(\"Review flesch reading score extracted..\"+ str(np.matrix(data_review_flesch_reading_score).shape))\n",
    "\n",
    "       \n",
    "    data_pos_sentiment_score, data_neg_sentiment_score, data_obj_sentiment_score = get_review_sentiment_score(dataset)\n",
    "    print(\"Review Sentiment scores extracted..\")\n",
    "    \n",
    "    data_summ_pos_sentiment_score, data_summ_neg_sentiment_score, data_summ_obj_sentiment_score = get_summary_sentiment_score(dataset)\n",
    "    print(\"Summary sentiment scores extracted..\")\n",
    "    \n",
    "    cosine_similarity_review = get_cosine_similarity_review(dataset, tf_idf_vectoriser, ideal_vector)\n",
    "    print(\"Cosine Similarity calculated..\")\n",
    "\n",
    "    feature_set = [\n",
    "        np.ones(len(dataset)),\n",
    "        ratings,\n",
    "        square_ratings,\n",
    "        total_helpfulness_votes,\n",
    "        review_word_count,\n",
    "        review_sentence_count,\n",
    "        review_word_allcaps_count,\n",
    "        review_char_count,\n",
    "        item_rating_deviation,\n",
    "        summary_word_count,\n",
    "        summary_word_allcaps_count,\n",
    "        user_review_experience,\n",
    "        review_readability_index,\n",
    "        review_posneg_diff,\n",
    "        review_stopwords,\n",
    "        summary_neg_words,\n",
    "        summary_specialchar_count,\n",
    "        outOf_low,\n",
    "        outOf_mid1,\n",
    "        outOf_mid2,\n",
    "        outOf_high,\n",
    "        rating_bad, \n",
    "        rating_ok, \n",
    "        rating_good,\n",
    "        unixTime,\n",
    "        review_pos_words,\n",
    "        review_neg_words,\n",
    "        data_review_flesch_reading_score,\n",
    "        data_pos_sentiment_score,\n",
    "        data_neg_sentiment_score,\n",
    "        data_obj_sentiment_score,\n",
    "        data_summ_pos_sentiment_score,\n",
    "        data_summ_neg_sentiment_score,\n",
    "        data_summ_obj_sentiment_score,\n",
    "        cosine_similarity_review\n",
    "    ]\n",
    "    dataset = np.stack(feature_set, axis=1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.shuffle(high_dataset)\n",
    "train_high_dataset = get_features(high_dataset)\n",
    "#print(train_high_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#np.random.shuffle(low_dataset)\n",
    "train_low_dataset = get_features(low_dataset)\n",
    "#print(train_low_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_feature_count = 29\n",
    "\n",
    "P = PCA(reduced_feature_count)\n",
    "P.fit(train_high_dataset)\n",
    "train_high_dataset = P.transform(train_high_dataset)\n",
    "print(train_high_dataset.shape)\n",
    "\n",
    "P1 = PCA(reduced_feature_count)\n",
    "M = np.mean(train_low_dataset.T, axis=1)\n",
    "train_low_dataset = train_low_dataset-M\n",
    "P1.fit(train_low_dataset)\n",
    "train_low_dataset = P1.transform(train_low_dataset)\n",
    "print(train_low_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_high_helpfulness = []\n",
    "for data_point in high_dataset:\n",
    "    data_helpfulness = data_point[index['helpful']]\n",
    "    train_high_helpfulness.append(data_helpfulness[0] * 1.0/data_helpfulness[1])\n",
    "train_high_helpfulness = np.matrix(train_high_helpfulness).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_low_helpfulness = []\n",
    "for data_point in low_dataset:\n",
    "    data_helpfulness = data_point[index['helpful']]\n",
    "    train_low_helpfulness.append((data_helpfulness[0] * 1.0+1)/(data_helpfulness[1]+1))\n",
    "\n",
    "train_low_helpfulness = np.matrix(train_low_helpfulness).T\n",
    "print (\"Extracted helpfulness score for \" + str(len(train_low_helpfulness)) + \" data points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_high_x = train_high_dataset[:int(0.5*len(train_high_dataset))]\n",
    "valid_high_x = train_high_dataset[int(0.5*len(train_high_dataset)):]\n",
    "train_high_y = train_high_helpfulness[:int(0.5*len(train_high_helpfulness))]\n",
    "valid_high_y = train_high_helpfulness[int(0.5*len(train_high_helpfulness)):]\n",
    "\n",
    "print (train_high_x.shape)\n",
    "print (valid_high_x.shape)\n",
    "print (train_high_y.shape)\n",
    "print (valid_high_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_low_x = train_low_dataset[:int(0.4*len(train_low_dataset))]\n",
    "valid_low_x = train_low_dataset[int(0.4*len(train_low_dataset)):]\n",
    "train_low_y = train_low_helpfulness[:int(0.4*len(train_low_helpfulness))]\n",
    "valid_low_y = train_low_helpfulness[int(0.4*len(train_low_helpfulness)):]\n",
    "print (train_low_x.shape)\n",
    "print (valid_low_x.shape)\n",
    "print (train_low_y.shape)\n",
    "print (valid_low_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ElasticNet Regressor\n",
    "\"\"\"\n",
    "predictor_high = ElasticNet(alpha=0.09, l1_ratio=0.005)\n",
    "predictor_high.fit((train_high_x), (train_high_y))\n",
    "predict_high_y = predictor_high.predict((valid_high_x))\n",
    "#predict_high_y = predictor_high.predict((train_high_x))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest for high data set\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators =300, max_features = 0.9, random_state = 42)\n",
    "rf.fit((train_high_x), (train_high_y))\n",
    "predict_high_y = rf.predict((valid_high_x))\n",
    "phigh = rf.predict(train_high_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting for low data set\n",
    "\n",
    "params = {'n_estimators': 300, 'max_depth': 5, 'min_samples_split': 3, 'loss': 'ls', 'learning_rate': 0.1}\n",
    "predictor_low = ensemble.GradientBoostingRegressor(**params)\n",
    "predictor_low.fit((train_low_x), (train_low_y))\n",
    "predict_low_y = predictor_low.predict((valid_low_x))\n",
    "plow = predictor_low.predict(train_low_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Linear Regression Model\n",
    "predictor_low = linear_model.LinearRegression()\n",
    "predictor_low.fit((train_low_x), (train_low_y))\n",
    "predict_low_y = predictor_low.predict((valid_low_x))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"# Polynomial Regression Model\n",
    "predictor_low = svm.SVR(degree=3, C=.1, epsilon=.01)\n",
    "predictor_low.fit((train_low_x), (train_low_y))\n",
    "predict_low_y = predictor_low.predict((valid_low_x))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RBF Regression Model\n",
    "#cvalue = 0.1\n",
    "#predictor_low = svm.SVR(C=float(cvalue))\n",
    "#predictor_low.fit((train_low_x), (train_low_y))\n",
    "#predict_low_y = predictor_low.predict((valid_low_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "# Instantiate model with 1000 decision trees\n",
    "\"\"\"\n",
    "rf = RandomForestRegressor(n_estimators=100, max_features=1, random_state=42, oob_score=True, min_samples_leaf=5)\n",
    "rf.fit((train_low_x), (train_low_y))\n",
    "predict_low_y = rf.predict((valid_low_x))\n",
    "plow = rf.predict(train_low_x)\n",
    "rf.oob_score_\n",
    "y_oob = rf.oob_prediction_\n",
    "print(y_oob)\n",
    "print(cross_val_score(rf, train_low_y, y_oob))\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Absolute Error Test Data\n",
    "mae_high = skmetrics.mean_absolute_error(valid_high_y, predict_high_y)\n",
    "print (\"Mean Absolute Error of Predictor : \" + str(mae_high))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Absolute Error Test Data\n",
    "mae_low = skmetrics.mean_absolute_error(valid_low_y, predict_low_y)\n",
    "print (\"Mean Absolute Error of Predictor : \" + str(mae_low))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Absolute Error Train Data\n",
    "mae_high = skmetrics.mean_absolute_error(train_high_y, phigh)\n",
    "print (\"Mean Absolute Error of Predictor : \" + str(mae_high))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Absolute Error Train Data\n",
    "mae_low = skmetrics.mean_absolute_error(train_low_y, plow)\n",
    "print (\"Mean Absolute Error of Predictor : \" + str(mae_low))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Complete Dataset for Test prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN Linear Regression Model\n",
    "#predictor = ElasticNet(alpha=0.09, l1_ratio=0.005)\n",
    "#predictor.fit((train_high_dataset), (train_high_helpfulness))\n",
    "\n",
    "predictor = RandomForestRegressor(n_estimators=300, max_features=0.9, random_state=42, oob_score=True, min_samples_leaf=5)\n",
    "predictor.fit((train_high_dataset), (train_high_helpfulness))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'n_estimators': 300, 'max_depth': 5, 'min_samples_split': 3, 'loss': 'ls'}\n",
    "predictor_low = ensemble.GradientBoostingRegressor(**params)\n",
    "predictor_low.fit((train_low_dataset),(train_low_helpfulness))\n",
    "\n",
    "#predictor_low = RandomForestRegressor(n_estimators=100, max_features=1, random_state=42, oob_score=True, min_samples_leaf=5)\n",
    "#predictor_low.fit((train_low_dataset), (train_low_helpfulness))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_predict(m_predictor, x_test):\n",
    "    return m_predictor.predict(np.matrix(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = []\n",
    "data = pd.read_csv('test_data_us.csv')\n",
    "for line in data.values:\n",
    "    test_dataset.append(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_feature_set = get_features(test_dataset[:])\n",
    "test_feature_set = P.transform(test_feature_set)\n",
    "#print(test_feature_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "====================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Helpful.txt\", 'w')\n",
    "idx = 0 \n",
    "for l in open(\"pairs_Helpful.txt\"): \n",
    "    if l.startswith(\"userID\"):\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,i,outOf = l.strip().split('-')\n",
    "    \n",
    "    score = (np.abs(test_predict(predictor, test_feature_set[idx])[0]*100) + np.abs(test_predict(predictor_low, test_feature_set[idx])[0]*100))/2.0\n",
    "    pred = str(score)[:4]+\"%\"\n",
    "    \n",
    "    predictions.write(u + '-' + i + '-' + str(pred) + '\\n')\n",
    "    idx += 1\n",
    "predictions.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
